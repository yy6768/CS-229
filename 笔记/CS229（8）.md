# CS229（8）

## 课程

- Bias / Variance
- Regularization
- train dev test
- Model selection



## Bias / Variance

- Bias：

  线性函数在拟合上出现较大的偏差

![image-20230409084240831](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230409084240831.png)

- Variance：

​	高阶函数在拟合上出现较大的方差

​	![image-20230409084438809](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230409084438809.png)

- GPU的训练可以训练出大模型，但是如果添加了过多的高维特征很容易训练出过拟合的训练结果





## Regularization

- $min_\theta \frac 1 2 \sum\limits_{i = 1}^m ||y^i - \theta^Tx^i||^2 + \frac \lambda 2 ||\theta||^2$
- 尾部的$\frac\lambda 2 ||\theta||^2$就是一种正则项，他惩罚了过大的参数，使得曲线变得更加平滑

- $\lambda$变大，曲线会变得非常平滑，$\lambda$变小，曲线会更加波动

- 支持向量机不会过度拟合，而是疯狂的添加特征，理论上证明了$min||w||^2$的效果与$\lambda||、theta||^2$的效果类似

Q：是否需要将所有参数都正则化？

A:并不一定，举一个详细的例子

- 朴素贝叶斯的文本分类问题中有10000个词，那么对应10000个参数，如果全部参与正则，那么对应10000个lambda，相当于需要选择20000个参数，这无疑增加了模型的困难程度，会在交叉检验的部分近一步讨论选择参数的问题

Q:怎样使不同特征的$\theta$的比例相同？（提问者描述了一些自己观点但是听不清）

A:对于不同规模的数据，需要进行预处理，标准化成一致的规模，以便于他们有相同的值范围和方差

Q：	为什么支持向量机一般不支持正则化表达，是因为参数都是较小的浮点数还是因为最小化惩罚W(没太理解)

A:形式化验证更依赖于后者，如果大部分数据是较为分散的，有较大的functional margin，那么SVM的复杂度较低，这样的SVM不太可能过拟合。



Q:（没听清）

A:高偏差往往就意味着欠拟合，大多数情况下两者的说法都是等价的。但是也有高偏差高方差的模型。模型会有很高的复杂度，但是也不能很好的拟合数据（如下图）

![image-20230409100352246](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230409100352246.png)



## 两个统计学派

- S是training Set

- 目标是找到$p(\theta|S) = \frac {p(s|\theta)p(\theta)}{p(s)} $

- $argmax_{\theta} = p(\theta|S) = argmax_{\theta} p(s|\theta)p(\theta) $

  

![image-20230409100803188](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230409100803188.png)



两个统计学派：

- 频率派：$p(s|\theta) \sim MLE$
- Bayes派 $p(s|\theta) 预先不可知p(\theta)大部分情况是gaussian- MAP$



![image-20230409101742908](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230409101742908.png)

上图展示了模型复杂度和错误率的关系

- 模型复杂度越高，train error会下降
- 但是通用测试的效果会变差

![image-20230409101910599](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230409101910599.png)

- 当lambda过大，会出现欠拟合，lambda过小会出现过拟合，需要找到一个lambda 使得两条曲线的差距足够小



## Train/dev/test 

- 10000个样本
- S->$S_{train},S_{dev},S_{test}$
- 在$S_{train}$上训练模型（模型有着不同维度的参数），获得一些训练好的模型$h_i$
- 在$S_{dev}$上测量误差，在dev上选择最优的模型
- 选出的模型在单独的测试集上进行检测

Q:dev和test上测试的结果差距很大吗？

A:一般相差不多，取决于规模，举个例子：

![image-20230409103854520](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230409103854520.png)

获得5阶多项式，但是有可能在dev集上测试仅仅是因为5阶多项式运气好，所以最后获得最好的结果，事实上有可能数据集仍然有偏差，最后实际上3,4,5阶多项式结果都一样，所以仍然要在test集上验证结果



Q:听不清

A:有很多benchmark的测试框架，当test的集合很大时，过度拟合的可能性很少，但是测试集小时，有可能会有过度拟合。CIFAR的数据集很少，现在一些研究者质疑因为太小了，所以有没有可能过拟合



### 划分train/dev/test的经验

- 73分经验，7分train，3分test(不选择模型)
- 622经验 ，6分train， 2分dev，2分test（选择模型）
- 当数据集非常巨大时，分给dev和test的比例会缩小

- 选择足够大的dev和test让训练集能够使不同算法能够产生足够的差异即可，满足这点的情况下，划分更多数据给train



## hold-out cross validation

- dev set  = cross-validation test
- 只有非常小的数据集（100个样本）
  - Strain有70,30个dev（医疗数据）



### k-fold cv

- 把数据集分为k个子集
- 将k-1用于训练，1份用于测试，迭代k次
- 最终得到k次测试的平均误差



### 留一法

- 相当于k = 1的k折交叉检验
- 当数据集比较小的时候比较适用



Q:既然有k个估算值，可以衡量这k个估算值的方差吗？

A:这k个估算值是线性相关的，有一篇论文从理论上尝试解释这k个值的相关性，但实际上不建议从这k个值的相关性提取相关信息



Q:是否需要在dl算法中使用k折交叉检验

A:数据规模小的时候可以，数据规模较大的时候，因为dl算法训练时间较长，所以一般不太实用，所以一般使用其他算法来进行改进（transfer  learning，more heterogeneity of inputs features ）



Q:k折交叉检验中的average是什么意思

![image-20230409112703063](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230409112703063.png)



测试误差的平均值，是对外部循环的平均值取值



Q:F1是不是意味着平均值

A：yes， F1很复杂，将会在下周五的讨论中详细说明

Q：如何对集合中的数据采样？

A:通过某种随机的概率分布进行采样，或者随机洗牌，现在的趋势尝试在不同场景对同一个算法进行测试，（手机语音和智能麦克风进行测试），如果想要了解更多关于数据采样的信息可以关注CS230和《Machine learning yeilding》



##  Feature Selection

- 如果有大量的特征，减少特征数是一个降低过拟合的有效方式（文本分类信息中的停用词，计算机视觉中的一些无用背景像素）
- 步骤：
  - 迭代，初始时$\mathcal{H} = \empty$
    - 每一次尝试添加一项特征i，并选择在dev中表现最好的的特征$i$加入到H中



