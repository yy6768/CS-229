# Lecture 10

## 课程内容

- 决策树
- 集成学习
- Bagging
- Random Forests
- Boosting



## Decision Tree

### 案例

- 滑雪-横坐标时间，纵坐标纬度

![image-20230414201348349](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230414201348349.png)

- 线性模型很难处理，因为数据集是线性不可分离的
- SVM可以处理
- 但是对于决策树来说将会变得非常简单

### Greedy Top-Down Recursive Partitioning

- 每一次将问题划分为多个子空间
- 子空间可以递归的进行划分

![image-20230414201649370](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230414201649370.png)

- 如何选择这些划分？
  - 定义损失$L(R)$表示在区域(节点)R上的损失
  - 给定C 个class，定义$\hat p$是proportion of examples in R that are of class c
  - $L_{misclass} = 1-max\ \hat p_i$ 
  - 减少损失$max\ L(R_p) - (L(R_1) + L(R_2))$
    - Rp 表示父节点
    - R1左节点，R2右节点
    - 希望可以最小化划分造成的损失增量



### Misclassification Loss Issues

![image-20230414202736237](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230414202736237.png)

- 右边的分类比左边的分类更好，因为显然获得了一个更均匀的正类的划分（500比200要多）
- 如果按照上述LOSS，两者的损失函数相同
- 根据这个思想采用交叉熵作为损失函数$L_{cross} = \sum\limits_c\hat p_clog_2\hat p_c$



![image-20230414204314732](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230414204314732.png)

- 图中可以明显的展示：
  - 交叉熵是一个凸函数
  - 真正的损失是曲线和左右儿子的连线的中点的差



Q:如何分配不均会怎么样，在曲线上会怎样

A:这是一个很好的问题，因为你的曲线是严格凸出的，所以只要你不选择相同的点，子节点就一定在父节点下方，得到正确的损失函数



![image-20230414205130827](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230414205130827.png)

- 对比下来，错误分类函数就显得不那么适用
- 当分类之后的子节点刚好位于分段函数的同一边时，此时Loss=0，所以很难区分分类是好是坏



- 另外还有一种损失函数Gini $\sum\limits_{c}\hat p_c (1-\hat p_c)$



## Regression Tree

### 案例：降雪量

![image-20230414205541328](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230414205541328.png)

- 每个区域的预测值：$\hat y_m = \frac{\sum\limits_{i\in R_m}y_i}{|R_m|}$
- Squred Loss：$ L = \frac {\sum\limits_{i\in R_m}(y_i - \hat y_i)^2}{|R_m}$

![image-20230415085400409](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230415085400409.png)

Q:如何遍历每个区域并划分成子区域

A:进行暴力搜索也算是比较高效的，待会会介绍具体细节



### Categorical Vars（分类变量）

- 如果有q个变量可能会有$2^q$个拆分
- 计算每个变量下有多少个正类，然后进行排序可能是最好的选择



### 决策树总结

- 在极限情况下，对于每一个点你都有一个单独的区域
- 决策树是一个极高方差的模型，我们可以进行一定的正则化



## Regularization of DTs

（1) 设置min leaf size

（2）设置max depth

（3）设置 max number of nodes

（4）设置min decrease in loss（设置最小损失量）

（5） 后向剪枝（misclassification with validation set)



Q：能够更详细的解释（4）吗？

A:在划分前：$L(R_p)$,划分后：$L(R_l)+L(R_r)$,如果划分前和划分后的差值不够大，那么我们认为这种分解基本没有什么好处，所以（4）的方法其实就是设置一个损失差的最少阈值。（课上说不会使用这种方法，原因是：有些好的划分方式是要通过次好问题来进行划分的，如果设置了最小损失阈值，那么很有可能不能得到之后的最优问题）



### Runtime

- notation：
  - n-样本数
  - f- 特征数
  - d-树的深度 
- 预测时间：$O(d)<log_2 n$
- 训练时间:Each point is part of O(d) nodes，每一个点的开销是$O(f)$ ,总计开销是$O(nfd)$(很快）



### No additive structure（非加性模型）

![image-20230415093829881](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230415093829881.png)

- 决策树在这种线性结构上添加了很多划分区域，导致复杂很多



### 决策树总结（2）

优点

- 容易理解和实现
- 解释性强
- 可以处理Categorical Vars
- Fast

缺点：

- 高方差
- Bad at additive（加性模型的建模支持较差）
- 预测准确性通常很低



Q:交叉熵损失log是否必须以2为底

A:我很确定，在这个问题下交叉熵的log是否以2为低不重要，但是交叉熵最初在信息论领域中产生，信息论都是01字符串的传递，所以标准定义是以2为底



## 集成学习

- $X_i$:随机变量（RV random variable)是独立同分布（IID， independent identically distributed)
- $Var(X_i) = \sigma^2, Var(\overline x)= Var(\frac 1 n \sum_iX_i) = \frac {\sigma^2} n$

- 独立性假设通常不正确，所以只是假设$X_i$是同分布的，X的关联系数$\rho$，则平均值的方差为：

  $Var(\overline X) = Var(\frac 1 n\sum\limits_i X_i) = \frac 1 {n^2}\sum\limits_{i, j} Cov(X_i,X_j) \\ =\frac {n \sigma^2}{n^2}+ \frac{n(n-1)\rho\sigma^2} {n^2} (\rho的定义和协方差展开)\\= \rho \sigma^2 + \frac{1-\rho}{n}\sigma^2$

  ![image-20230415101931773](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230415101931773.png)



### 集成的方法

- kaggle上很多人通过将神经网络，SVM等方法集成在一起取得较好的成绩
- 几种方法：
  - different algorithm
  - different training sets
  - Bagging （random forest）
  - Boosting（adaboost，xgboost)



### Bagging-Bootstrap Aggragation

- Boostrap-统计学习方法中产生，最初的目的是测量某些估计量（如均值）的不确定性。
- 假设有一个总体P ，训练集S从P中采样,记为$S\sim P$
- 自助法假设P=S，创建一个新的集合Z，$Z\sim S$,作为新的训练集



训练流程

- Bootstrap samples $Z_1,Z_2,..,Z_m$
- Train model $G_m\ on\ Z_m$
- 集成模型取所有模型预测的均值：$G(m) = \frac {\sum\limits_{m=1}^MG_m(x)}{M}$



#### Bias-Variance Analysis

- $Var(\overline X) = p\sigma^2 + \frac{1- p}{M}\sigma^2$(M是自助法产生的训练集个数）
- Boostraping is driving down $\rho$
- M越大，协方差越小，就可以保证不容易过拟合



Q:可以定义出降低$\rho$的界限吗？

A:肯定有一个$\rho$的下界，但是没有形式化的数学公式来表明$\rho$的下降



- Bias 会轻度的上升（因为$Z$的大小比$S$小，损失了总体集合的特征）

Q：可以解释一下随机变量和算法之间的区别吗？

A:在high-level层次上看，算法是一种可以获取数据并进行预测功能的模型，随机变量则是从概率的角度给出某种输出，从某种程度上看模型可以看做某种内在的分布



#### DTs + Bagging(继承决策树构建随机森林)

- DT 是高方差，低偏差的算法，非常适合Bagging
- 随机森林：
  - 每一次划分，只考虑特征的子集进行划分
  - 优点在于降低了$\rho$



### Boosting

- 下降偏差
- Boosting的思想是对于每一次训练提高上一次分类错误的样本

![image-20230415105931001](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230415105931001.png)

降低分类错误的model的权重$log(\frac {1-err_m} {err_m})$

$G(X) = \sum\limits_{m} \alpha_mG_m$ 每一个$G_m$在重新加权的训练接的训练集中训练

在讲义中会更加详尽的讲述AdaBoost和XGBoost 