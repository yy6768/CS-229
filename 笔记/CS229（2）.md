# CS229（2）

## 课程内容

- Linear Regression
-  batch gradient descent





## 构建机器学习的步骤

![image-20230103200537096](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230103200537096.png)

Training Set：训练集

Learning algorithm：学习算法

h：假设

吴老师提出了一个问题：怎么表示h？



## Linear Regression

线性回归这种假设就变成了线性函数：$h(x) = \sum\limits_{i=1}^{n}\theta_ix_i, x_0 = 1$



$$\theta=\begin{bmatrix}{\theta_0}\\{\theta_1}\\{\theta_2}\\{...}\end{bmatrix}$$

$$x=\begin{bmatrix}{x_0}\\{x_1}\\{x_2}\\{...}\end{bmatrix}$$

### 术语

- $\theta$被称为**参数**，学习算法就是需要确定参数
- m被称为训练集的数量（x的行数）
- x表示输入（input features/attribute）
- y表示输出（output/target)

- $(x^{(i)},y^{(i)})$表示第i个训练集

- n表示监督学习中特征的数量



### 如何选择参数

目标：$h(x) \approx y$ 

符号：$h_\theta(x) (强调h是受参数\theta约束，一般h(x) = h_\theta(x))$

我们选择参数是希望两者差距最小，经验上选择平方差作为差值函数，表示为误差函数：

$J(\theta) = min_\theta \sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$

（下节课会介绍一般化的线性回归，和为什么是平方差作为损失函数）



## Gradient descent

从一些默认的参数值$\vec\theta$开始，持续减少$J(\theta)$的方法

![image-20230103203616149](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230103203616149.png)

求当前$J(\theta)$的梯度，沿着梯度小步前进，最终会进入极小值中（可能是局部极小值）

线性回归不会有局部极小值（稍后证明）

### 符号和公式

- $\theta_{ji} = \theta_{j} - \alpha\frac{\part J(\theta)}{\part\theta}(j = 0..n)$

- $\alpha$表示学习率

- 线性回归中：$\frac{\part J(\theta)}{\part\theta_j} = \frac{\part}{\part\theta_j}\frac 1 2(h_\theta(x)-y)^2\\=(h_\theta(x) - y)\frac{\part}{\part \theta_j}(h_\theta(x)-y)\\=(h_\theta(x) - y)\frac{\part}{\part \theta_j}(\theta_0x_0+\theta_1x_1+..\theta_nx_n-y)\\=(h_\theta(x) - y)x_j$(链式求导)





- 实际上还需要考虑多个训练集和：

  $\theta_j:=\theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}$



### Batch descent

朝向全局参数的下降方向

优点：能够朝着梯度快速下降的方向移动 

缺点：在大数据集中，需要计算所有梯度的总和，复杂度非常高



### Stochastic  descent

朝向局部参数下降的方向

优点：开销较小

缺点：不一定朝着梯度下降的方向前进



## Normal Equation（只适用于线性回归）

 符号：

- 定义梯度：$\nabla_\theta J(\theta)=\begin{bmatrix}{\frac{\part J}{\part\theta_0}}\\{\frac{\part J}{\part\theta_1}}\\{\frac{\part J}{\part\theta_2}}\\{...}\end{bmatrix} (\theta \in R) $



- 假设A是一个2*2矩阵：$A\in R^{2\times2}\\A=\begin{bmatrix}{A_{11} \ A_{12}}\\ {A_{21}\ A_{22}}\end{bmatrix}$

  我们可以定义$R^{2\times2}->R$函数,例如$f(A) = aA_{11}+bA_{12}^2$

  定义$\nabla_Af(A) = \begin{bmatrix}{\frac{\part f}{\part A_{11}} \ \frac{\part f}{\part A_{12}}}\\ {\frac{\part f}{\part A_{21}}\ \frac{\part f}{\part A_{22}}}\end{bmatrix}$，对于上述函数$\nabla_Af(A)=\begin{bmatrix}{a \ 2bA_{12}}\\ {0\ \ \ \ \ \ \ 0}\end{bmatrix}$



derivation:

1.  如果A是一个方阵（square），$A\in R^{n\times n}$
2. $tr\ A =\sum\limits_{i=1}^n A_{ii}$
   1. $tr\ A = tr\ A^T$
   2. 课后证明：如果定义$f(A)=tr\ AB(B是一个固定的矩阵)，那么可以得到\nabla_A f(A)= B^T$ 
   3. 课后证明：$tr\ AB = tr\ BA, tr\ ABC = tr\ CAB$
   4. 课后证明：$\nabla_A tr\ AA^TC = CA + C^TA$



定义：

- $X=\begin{bmatrix}{\_\_\_(x^{(1)})^T\_\_\_}\\{\_\_\_(x^{(2)})^T\_\_\_}\\{\_\_\_(x^{(3)})^T\_\_\_}\\...\\{\_\_\_(x^{(m)})^T\_\_\_}\end{bmatrix}$

- $X\theta=\begin{bmatrix}{h_\theta(x^{1})}\\{h_\theta(x^{2})}\\{h_\theta(x^{3})}\\...\\{h_\theta(x^{m})}\end{bmatrix}$

- $J(\theta) = \frac 1 2 \sum\limits_{i=1}^m (h(x^{(i)})-y^{i})^2=\frac  1 2(X\theta- y)^T(X\theta- y)$

- ![image-20230104225101552](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230104225101552.png)

(第三步运用$a^Tb = b^Ta$，第五步运用了$\nabla_x b^Tx = b$和$\nabla_x x^TAx= 2Ax$)

最后需要$\nabla_\theta J(\theta) = 0$，所以就有$X^TX\theta = X^T\vec y$(normal equaltion)

$\theta = (X^TX)^{-1}X^T\vec y$



当特征是线性相关是$ (X^TX)^{-1}$可以使用伪逆

