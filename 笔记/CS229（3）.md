# CS229（3）

## 课程内容

- locally weighted regression
- probabilistic interpretation
- logistics regression
- Newton method





### Locally weighted

- parametric/unparameteric：

  - para:固定的参数集大小
  - unparameteric：数据/参数数量随着训练集合增长

- locally weighted regression定义：

  - fit $\theta$ to minimize $L = \sum\limits_{i=1}^m w^{(i)} (y^{(i)}- \theta^Tx^(i))^2$

  - 其中$w^i = exp(-\frac {(x^i-x)^2} 2)$

  - 理解：如果x与$x^{(i)}越近，w越小，也就是其贡献的权重越小，模型可以更针对局部$

  - 对于分布的宽窄：引入宽带参数$\tau$

    $w^i = exp(-\frac {(x^i-x)^2} {2\tau^2})$,$\tau$越大说明拟合的数据越稀疏$\tau$会影响overfitting和underfitting

    - $\tau$越大，最终会获得很平滑的模型，而越小很难拟合

  - 一般当遇到特征数量少的时候使用local weight regression（计算量很大）



## 对线性回归的概率解释

- 为什么使用平方差？

  - 我们假设模型$y^{(i)} = \theta^Tx^{(i)}  + \epsilon^{(i)}(error : model effect random noise)$

  - 如果假设$epsilon$满足高斯分布：$P(\epsilon^i) = \frac 1{\sqrt{2\pi}\sigma}exp(-\frac {\epsilon^i}{2\sigma^2})$

  - 属于IID：独立（independent）相同（identify）的数据分布（data)

  - $P(y^i|x^i;\theta) = \frac 1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)- \theta^Tx^{i})^2}}{2\sigma^2})$

  - likelihood函数（似然函数）：$\zeta(\theta) = P(\vec y|x;\theta) = \prod\limits_{i = 1}^{m}P(y^{(i)}|x^{(i)};\theta)=\prod\limits_{i=1}^m\frac 1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)- \theta^Tx^{i})^2}}{2\sigma^2})$

  - log likelihood:对数似然函数：

    $l(\theta) = log \zeta(\theta) = m log\frac 1{\sqrt{2\pi}\sigma} + \sum\limits_{i=0}^m-\frac{(y^{(i)- \theta^Tx^{(i)}})^2}{2\sigma^2}$

  - 我们需要选取最大的$\theta$可能性，所以需要likelihood函数最大，也需要log likelihood函数最大，所以就是需要$\frac{(y^{(i)- \theta^Tx^{(i)}})^2}{2\sigma^2}$最小，也就是$J(\theta)$



## Classification

- $y\in{0,1}$,分类函数需要函数$h_\theta(x) \in [0,1]$
- logistic regression:$h_\theta(x) = g(\theta^Tx) = \frac 1 {1+e^{-\theta^Tx}}$ 

![image-20230109213918690](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230109213918690.png)

- 假设：
  - $P(y=1|x;\theta) = h_\theta(x)$
  - $P(y = 0|x;\theta) = 1 - h_\theta(x)$
- 结合上述两式子可以推出：$P(y|x;\theta) = h(x)^y(1-h(x))^{1-y}$

- 仍然需要极大似然函数：$\zeta(\theta) = P(\vec y|x;\theta) = \prod\limits_{i = 1}^{m}P(y^{(i)}|x^{(i)};\theta) =\prod\limits_{i=1}^mh_{\theta}(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}$
- log likelihood:$l(\theta) = \sum\limits_{i=1}^my^{(i)}logh_\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\theta(x^{(i)}) $\
- 使用batch gradient 极大化log likelihood

![image-20230109220409876](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230109220409876.png)



## Newton method

- 相比于gradient descent，牛顿法迭代次数更少，但是计算开销更大
- 定义：有一个函数f，需要寻找$\theta$使得=>$f(\theta) = 0$(找到$l'(\theta) = 0$)
- 切线和x轴的交点作为$\theta$下一次迭代的值，$\theta^{(i)} = \theta^{(i-1)} - \delta$,delta如图所示：![image-20230109221637713](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230109221637713.png)
  - $\frac {f(\theta^{i-1})}\delta = f'(\theta^{i-1})$

- 对于逻辑回归求解:$f(\theta) = l'(\theta)$,所以逻辑回归牛顿法：$\theta^i = theta^{i-1} - \frac {l'(\theta^{i-1})}{l''(\theta^{i-1})}$

- 牛顿法具有quadratic convergence：在一次迭代中，假设误差值为0.01，下一次迭代与真实值得误差会下降到0.0001(较少迭代得原因)

- 正式的：$\theta^i = theta^{i-1} + H^{-1}\nabla_\theta l$

  H是hessian矩阵：$H_{ij} = \frac{\part^2l}{\part\theta_i\part\theta_j}$(计算量很大)