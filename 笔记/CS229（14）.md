# CS 229 （14)

## 课程内容

- 非监督学习

  - K-means

  - Mixture of Gaussian
  - EM

## 聚类（K-Means）

![image-20230427151141461](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230427151141461.png)

- 随机选取聚类的中心
- 通过距离计算决定每个点为哪一个类
- 对于每一个类的点，计算均值作为下一次该类的中心
- 直到每一个类的中心都不发生改变



可证明：K-means是可以收敛的

- distortion function：$J(c,\mu) = \sum\limits_{i=1}^m||x^{(i)} - \mu_{c^{(i)}}||^2$

- 很明显k-means实际上就是对J进行坐标下降，而J存在极值，所以可能收敛
- J是一个非凸函数，所以可能下降到的机制是非全局最小值





K-means经常可以作用于异常检测



## 混合高斯模型

- 以一维为例

  ![image-20230427153956429](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230427153956429.png)

  数轴上有多个离散点，可以看出来可以用两个高斯模型对他们进行拟合

- 混合高斯模型

  - 假设latent（潜在的）随机变量Z满足：$p(x^{(i)} , z^{(i)} ) = p(x ^{(i)} |z ^{(i)} )p(z ^{(i)} ).$

  - z (i) ∼ Multinomial(φ) ，$x^{(i)}|z^{(i)} = j \sim \mathcal{N}(\mu_j,\Sigma_j)$

  - 每个高斯分布都有自己的协方差矩阵

  - 似然函数和参数拟合

    ![image-20230427154647795](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230427154647795.png)



## EM

- EM算法分为两步：

  - E步（期望步）：set $w_j^{(i)} = P(z^{(i)} = j|x^{(i)};\phi,\mu,\Sigma) $

    - 根据贝叶斯规则进行转换

      $\frac {p(x^{(i)}|z^{(i)} = j)p(z^{(i)})}{\sum\limits_{l = 1}^kp(x^{(i)}|z^{(i)} = l;\mu,\Sigma)p(z^{(i)} = l;\phi)}$

    - 前者是高斯分布，后者是多项式分布

    - $w_j^{(i)}是sigmoid$的高度（图中红线）

      ![image-20230427155528978](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230427155528978.png)

  - M步：使用最大似然公式（指标函数的期望值等于概率）

    ![image-20230427155906012](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230427155906012.png)

  - 至少收敛于局部最优



### EM算法的数学推导

#### Jensen 不等式

- 令f为某个凸函数（连续可二阶导，且f''>0）
- $f(EX) \le E[f(x)]$

![image-20230427162219353](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230427162219353.png)

- 如果f''>0 （f是严格凸函数（不是仿射函数）） 那么E[f(x)] = f(EX) =》f是一个常数

- x=E[x] with probability 1
- log函数是凹函数，我们将进行一定的转换（负的凸函数就是凹函数）



### 密度估算问题

- model for $p(x,z;\theta)$,只有x

- $l(\theta) = \sum\limits_{i = 1}^m log p(x^{(i)};\theta)\\ =\sum\limits_{i = 1}^mlog\sum\limits_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)$



- 在EM算法的E步，期望对于$\theta$构建一个期望下限
- E步就是构建绿色曲线

![image-20230427163108078](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230427163108078.png)

- M步寻找绿色曲线的最大值，最大值点记为$\theta'$
- 每一次迭代，拟合的曲线的最大值点都会像真实的高斯模型的最高点靠近

![image-20230427163248638](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230427163248638.png)

- 目的：

  ![image-20230427163534387](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230427163534387.png)

- $Q(z^i)$是某种概率分布 $\sum\limits_{z^(i)} Q^{(i)}=1$

- 第二步到第三步使用了凹函数版本的Jensen'sinequality

![image-20230427163837956](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230427163837956.png)

- 在给定的EM迭代中（$\theta$确定），我们的目标函数是$log \mathbb{E}_{z^{(i)}\sim Q}[\frac {p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}] = \mathbb{E}_{z^{(i)}\sim Q}[log\frac {p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}] $

- 为了让上述等式成立，我们需要$\frac {p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} = constant$
- 我们令 $Q(z^{i}) \propto p(x^{(i)},z^{(i)};\theta)$



![image-20230427165220913](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230427165220913.png)