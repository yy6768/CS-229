# CS229（6）

## Topic

- Naive Bayes
- advice for applying ML
- SVM



## Naive Bayes

- 问题：适用于文本分类问题

- 模型：
  - 参数：$\phi_{j|y = 1} = p(x_j = 1 | y = 1), \phi_{j|y=0}=  p(x_j =1 |y = 0), \phi_y = p(y = 1)$
  - 联合似然：
    - $\mathcal{L}(\phi_y, \phi_{j|y = 0},\phi_{j|y = 1}) = \prod\limits_{i = 1} ^ np(x^{(i)}, y^{(i)})$
  - 最大似然后，得到的参数：
    - $\phi_{j|y = 1} = \frac{\sum\limits_{i=1}^nl\{x_j^i =1 \and y^i =1\}}{\sum\limits_{i=1}^nl\{y^i = 1\}}$
    - $\phi_{j|y = 0}=\frac{\sum\limits_{i=1}^nl\{x_j^i =1 \and y^i =0\}}{\sum\limits_{i=1}^nl\{y^i = 0\}}$
    - $\phi_{j} = \frac{\sum\limits_{i=1}^n l\{y^i = 1\}}{n}$
  - $p(y = 1|x) = \frac {p(x|y = 1)p(y = 1)}{p(x|y = 1)p(y = 1)+ p(x|y = 0)p(y = 0)}$通过是某个x=1| y=1 的概率为0



### 拉普拉斯平滑

- 问题：追踪斯坦福大学足球队的胜负情况，经过客场四连败，在下一场主场胜负情况预测将会怎样
- 如果不使用拉普拉斯平滑,朴素贝叶斯采用频率估计得到$p(x = 1) = \frac {\#(获胜)}{\#(失败)+\#(获胜)}= \frac 0 4 = 0$

- 拉普拉斯平滑就是分子+1 分母+2:$\frac {\#(获胜)+ 1}{\#(失败)+1+\#(获胜)+1} = \frac 1 6$
- **标准定义：对于随机变量$X\in \{1,2..k\}$ ,$p(x = j) = \frac {\sum\limits_{j =1}^m I\{x^{(i = j)}\}+1}{M+k}$**
- 对于朴素贝叶斯$\phi_{j|y = 0}=\frac{\sum\limits_{i=1}^nl\{x_j^i =1 \and y^i =0\} + 1}{\sum\limits_{i=1}^nl\{y^i = 0\}+2}$



###  multi event model（多项事件模型）

- 直至现在，贝叶斯模型只表达二进制问题，如何处理多分类问题？
- 如果遇到连续值，可以通过离散化解决

![image-20230329211953913](C:\Users\12587\AppData\Roaming\Typora\typora-user-images\image-20230329211953913.png)

分解成<400 > 400等类

- 朴素贝叶斯有一个问题就是，假设一个词出现了多次，因为它将邮件编码成one-hot模式，他丢弃了出现次次数的特征

  解决方案：使用multinomial-event 模型



multinomial event 模型

不采用编码成one-hot的方式，而是对词典进行编号，比如hot的词典编号是100，drag的编号是1100，x（输入）会记录每一个单词的编号值





- 似然函数：$p(x, y) = \prod\limits ^n _{j = 1}P(x_j|y)$
- 参数:
  - $\phi_{k|y =1} = \frac{\sum\limits_{i = 1}^n\sum\limits_{j = 1}^{d_i}I\{x_j^i = k \and y^i = 1\}}{\sum\limits_{i = 1}^nI\{y^i = 1\}d_i}$
  - $\phi_{k|y =0} = \frac{\sum\limits_{i = 1}^n\sum\limits_{j = 1}^{d_i}I\{x_j^i = k \and y^i = 0\}}{\sum\limits_{i = 1}^nI\{y^i = 0\}d_i}$
  - $\phi_y = \frac{\sum\limits_{i=1}^nl\{y^i =1\}}{n}$
- 拉普拉斯平滑：会在$\phi_{k|y = 1}和\phi_{k|y =0}$的分子加上1，分母加上$|V|(词汇表)$



Q:如果遇到稀有字符字母表中没有怎么办

A:

- 其中一种方式可以丢掉
- 另外就是添加一个特殊标记UNK，UNK单独作为一个单词



Q：I是什么

A:Indicate 函数



### 什么时候运用Naive Bayes

- 多数情况下逻辑回归会比朴素贝叶斯好
- 朴素贝叶斯的好处在于易于计算和实现，不需要使用梯度计算等复杂运算



## Support Vector Machine

![image-20230329215501791](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230329215501791.png)

如何处理非决策分类分界：

- 引入高维特征进行逻辑回归
- 使用支持向量机



支持向量机是：

- 一种相对完善的算法，已经用很完善的工具包来进行计算
- 不需要考虑调试超参数



问题：最优线性边界分类器

- 模型：$h_\theta(x) =g(\theta^Tx)$进行分类会预测正类和负类（0和1）
  - prefect "1"：$\theta^Tx \ge 0$ => $\theta^Tx^i >> 0$
- function margin:思想是$\theta^Tx^i >> 0当为正类时，反之\theta^Tx^i <<0 当为负类时$



### 几何余量（geometric margin）

![image-20230329221840313](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230329221840313.png)

- 绿线明显距离正类和负类更远，那么它的几何余量更大



### 符号

- $y\in \{-1, +1\}$
- $h(w^Tx + b) = h_\theta(x)(其中\theta_0 = b)$是模型，输出{-1, +1}
- $$g(z) = \left\{ \begin{matrix} 1 \ if\ z \ge 0\\ -1\ otherwise \end{matrix} \right.$$



### functional margin

定义：针对一个超平面$（w^i, b^i)$满足：

- $如果 y^i = 1 ,期望 我w^Tx^i +b >>0$
- $如果 y^i = -1 ,期望 我w^Tx^i +b <<0$
- 所以有functional margin ：$\hat\gamma^i = y^i(w^Tx^i +b)$

整个训练集的functional margin：

$\hat \gamma = min \hat \gamma^i$



性质：当w和b同时乘上一个系数，决策边界并未改变

不妨设||w|| = 1，这样可以更好地确定超平面





### geometric margin

![image-20230329223859340](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230329223859340.png)

关键是找到：x点与超平面的距离（geometric margin）

- 定义：$\gamma^i = \frac {y^i(w^ix^i +b ^i)}{||w^i||}$

- 当$||w^i|| = 1$=》functional margin = geometric margin

- 对于全训练集的几何余量：

  $\gamma = min_i \ \gamma^i$





模型目标

训练参数$w, b$使得最大化$\gamma$

$max_{r,w,b}\ \gamma\ st. \frac {y^i (w^Tx^i +b^i)}{||w||}\ge \gamma $

简化：

$min_{w,b}  ||w||^2\\st.y^i(w^Tx^i +b)\ge 1$
