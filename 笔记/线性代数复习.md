# 线性代数复习

## 矩阵和向量

1. 符号：

   1. vector：$\vec v \in \R^n  => \begin{bmatrix} x_1 \\ x_2\\ x_3\\x_4\\..\\x_n\end{bmatrix}$
   2. matrix :$A\in \R^{m \times n} => \begin{bmatrix}
      {a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\
      {a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\
      {\vdots}&{\vdots}&{\ddots}&{\vdots}\\
      {a_{m1}}&{a_{m2}}&{\cdots}&{a_{mn}}\\
      \end{bmatrix}$
   3. Identity:单位矩阵：$I^n = \begin{bmatrix}
      1&0&{\cdots}&0\\
      0&1&{\cdots}&0\\
      {\vdots}&{\vdots}&{\ddots}&{\vdots}\\
      0&0&{\cdots}&1\\
      \end{bmatrix} $
   4. diagonal matrix:对角矩阵D
   5. 对称矩阵：$S^T = S$
   6. Trace(A) (迹)：$\sum\limits_{i = 1}^n A_{ii}$

2. 线性代数计算

   1. Inner product（内积）：$v\in \R^n, u \in \R^n=>v^Tu = <v,u> \in R = \sum\limits_{i = 1} ^ n v_iu_i$

   2. out product(外积)：$v\in R^m,u \in R^n=> vu^T = 一个m*n的矩阵$

   3. 加法

      两个矩阵相加的秩：rank(A+B) $\le$ rank(A) + rank(B)

      证明可以通过极大线性无关组

   4. rank 秩

   5. 乘积：

      -  $M \in \R^{m \times n} ,x\in \R^n=> Mx = a  \in \R^{m}$
   
      - $M\in \R^{m\times k}, N\in \R^{k \times n} = >X= MN \in  \R^{m \times n}$
      - $A^TA$

## 机器学习中的线性代数

- 表达数据
- 概率学中一组随机变量
- 微积分中的优化问题

![image-20230302215757056](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230302215757056.png)

值得注意的四种矩阵：

Gradient 梯度， Hessian 矩阵（二阶导数）

jacob 矩阵， T矩阵

- 核函数





### projection 投影

- 向量对向量的投影$proj(\vec b, \vec j) = \begin{bmatrix} \frac {\vec v \vec v^T}{\vec v^T \vec v} *\vec b\end{bmatrix}$

- 如果有平面（矩阵）由若干个向量确认，假设这个矩阵为V

  那么可以得到向量在这个平面上的投影：$proj(\vec b, V) =[V(V^TV)-1V^T]\vec b$

根据投影我们可以更好的理解线性回归中为什么$\theta = (X^TX)-1X^T y$





### decomposition 

- eigen decomposition 
- SVD



spectrum