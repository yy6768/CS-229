# CS229 (7)

## 课程内容

- 优化问题
- representater theorem（表达高维数据的理论）
- kernals 
- example of kernals



## 复习

- geometry margin	
- optimial margin classifer - > SVM



## The optimal margin classifier

- 起始的训练目标：$max_{\gamma, w, b} \gamma \ \  st. {y^i(w^Tx^i + b)} \ge \gamma \\||w|| = 1$

- **但是$||w|| = 1$不是一个凸优化约束**（不知道，感觉应该是一个凸优化约束），所以需要移除限制条件$st. ||w|| = 1$，因为如果分类器是$y = g(w^Tx +b)$

  如果 $w^T = [2, 1]$, $y = g([2,1]x+b) = g([20,10]x + 10b)$，所以可以同时放大缩小||w||和b的值，得到

- $max_i \gamma \ \  st. \frac {y^i(w^Tx^i + b)}{||w^i||} \ge \gamma $（st.表示约束条件，这里表示任何几何余量都要大于等于gamma， 我们的训练目标是最大化最小几何余量的值）

- 经过geometry margin和functional margin 可以相互转换$max_{\hat\gamma,w, b} \frac{\hat \gamma}{||w||} \\st. y^i (w^Tx^i + b)\ge \hat\gamma, i =1 ... n $

- 经过||w||=1 去除的思想，我们也可以限制functional margin：$\hat \gamma = 1$，得到

  $min_{w,b} \frac 1 2||w||^2\\s.t. y^i(w^Tx^i + b) \ge 1, i= 1,....,n$

- 上述优化可以最终得到一个**optimal margin classifier**，这个问题可以被商业的二次规划软件解决



## Representater theorem

- $x ^i \in \R ^{100} ，如果是\R^{10000}怎么办$

- suppose 可以被写为：$ w = \sum\limits^m_{i = 1} \alpha_i y^i x^i(y^i = +1 | -1)$

- 为什么这个假设是合理的：

  -  从直觉#1上，在线性回归中$$w := w - \alpha(学习率与上述\alpha无关)(h_\theta(x ^i) - y^i)x^i$$,最终w会被优化为某种训练集$x_i$和$y_i$的线性组合

  - 从直觉#2上，w决定决策方向，b决定决策偏移，

    ![image-20230407145041387](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230407145041387.png)

    考虑所有样例分布在3维空间，两类事物x和o的样例$x_3 = 0$分类平面入上图所示，$w_3 = 0$, $w_3$由$x_3$决定，所以可以体现出这一假设

  - 这个假设实际上在推导非常复杂，需要形式化证明

- $w = \sum\limits_{i =1}^m y^ix^i\\min_{w, b} \frac 1  2 ||w||^2 s.t.y^i(w^Tx^i)\ge 1 \rightarrow \\ min \frac 1 2(\sum\limits_{i = 1}^m\alpha_iy^ix^i)^T(\sum\limits_{i = 1}^m\alpha_iy^ix^i))= \\min \frac 1 2\sum\limits_{i = 1}^m\sum\limits_{j = 1}^m\alpha_i\alpha_jy^iy^jx^{i^T}x^j(x^{i^T}x^j 可以写作<x^i,x^j>表示内积)  \\ s.t.y^i((\sum\limits_{j = 1}^m\alpha_jy^jx^j)^Tx ^i + b) \ge 1\\ \rightarrow s.t. y^i(\sum\limits_{j = 1}^m\alpha_jy^j<x^j,x^i> + b) \ge 1$



- 对偶优化问题：上述式子还可以优化成更简单的版本，具体见讲义

![image-20230407151624249](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230407151624249.png)



训练过程

1. 解出$\alpha ^i ,b$
2. 进行预测：$h_{w,b}(x) = g(w^Tx +b) = g((\sum\limits_{i = 1}^m \alpha_iy^ix^i)^Tx+b)$



## kernal trick

1. write algorithm in terms a$<x,z>$
2. let there be some mapping from $x \rightarrow \phi(x)(添加高维度特征)$
3. find a way to compute $k(x,z) =\phi(x)^T \phi(z)$
4. replace <x,z>   in algorithm with K(x,z)



Q：没听清

A：会在接下来介绍，偏置和归纳……



#### example

x= $\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}$

$\phi(x) = \begin{bmatrix}x_1x_1\\x_1x_2\\x_1x_3\\x_2x_1\\x_2x_2\\x_2x_3\\ x_3x_1\\x_3x_2\\x_3x_3\end{bmatrix}$

- 假设有n维，因为$n^2$个元素，需要$O(n^2)$才能计算

需要找出方法计算$k<x,z> = \phi(x)^T\phi(z) = (x^Tz)^2$ 时间复杂度下降到O(n)(x的规模是n， z的规模也是n，每一次枚举一个元素相乘即可，最后得到结果再平方)



- $k(x,z) = （x^Tz+c)^2 => 添加\sqrt {2c}x到\phi(x)中$

  ![image-20230407155353194](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230407155353194.png)

- $k(x,z) = (x^Tz +c)^d$O(d)时间就可以计算



Q:听不清

A:内核函数仅适用于可视化映射，每种kernal function定义一种特征变化，当特征变化发生改变，会得到完全不同的kernal函数



Q:在高维空间$\phi(x)$必须是线性可分的吗？

A:现在是这样假设的，之后会进行修正



## How to make kernals

- 如果$x,z$是“similar"的，K(x,z)会很大，x，z"dissimilar"，K(x,z)很小
- 是否存在$\phi(x) st.K(x,z) = \phi(x)^T\phi(z)$

- 需要$K(x,x) = \phi(x)^T\phi(x) \ge 0$

- K可以看做一个矩阵（内核矩阵） =>$k_ij = K(x^i,x^j)$，如果要有效，要满足：

  ![image-20230407161640378](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230407161640378.png)

- 如果要有效，K必须是半正定的$K\ge 0$



**Theorem (Mercer). Let $K:\R^d × \R ^d → \R$ be given. Then for K to be a valid (Mercer) kernel, it is necessary and sufficient that for any $\{x (1), . . . , x(n)\}$, (n < ∞), the corresponding kernel matrix is symmetric positive semi-definite.**

必须是半正定矩阵



常见的核函数：

- 线性核$k(x,z) = x^Tz$

- 高斯核$K(x,z) = => \phi(x)\in \R^\infty$,对应所有单项式特征



应用：对于任何使用内积的算法就可以使用核函数



## 软间隔SVM

- 之前假设数据是线性可分离的，但是如果数据集中出现了噪声，就无法实现数据可分离了

- L1 norm soft margin svm 算法（L1范数软间隔SVM)

  $min_{w,b, \xi} \frac 1 2 ||w||^2 + C\sum\limits_{i =1 }^m \xi_i \\s.t. y^i(w^tx^i+b) \ge 1 - \xi_i\\\xi_i \ge 0$

- 如果$\xi_i = 0$，那么就是硬间隔SVM

- 硬间隔的SVM不允许任何异常值，这也导致了会出现过拟合的异常现象，但是L1软间隔不同，当在异常值接受范围内，它不会去调整异常

- C在后面的Lecture会介绍如何选择（C越大容错性越小）



## SVM的应用

- 手写数字分类
- 蛋白质序列分类器