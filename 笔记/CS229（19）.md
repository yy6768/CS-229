# CS 229 （19）

## 课程内容

- 离散化
- 模型/仿真
- 有限价值迭代



## 建模-环境

- 案例1：对汽车建模
  - 汽车动力学模型：六元组

![image-20230523113030992](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230523113030992.png)

- 案例2：直升机
  - 12元组：添加了roll，pitch，yaw（滚动角，俯仰角，偏航角）等
  - ![image-20230523113410507](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230523113410507.png)
- 案例3：倒立摆
  - $x 、\theta 、\dot x、 \dot \theta$

大多数环境的状态 state： $s \in R^n$，也就是连续且无穷的状态





## 离散化

- 上述的状态都是无穷的，对于我们之前学习的有限状态的强化学习算法无法很好的处理
- 离散化是一种朴素的想法来解决这些问题并应用到价值和策略迭代中

![image-20230523114158083](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230523114158083.png)

- 缺点：
  1. 拟合出的值函数并不平滑
  2. 维度问题：$\R^n => $ 每个维度变成k个值，状态空间大小$k^n$，状态爆炸

- 使用范围：
  - 当你的状态仅有2-3维时，离散化是比较合理的方法
  - 当4-6维时，需要谨慎的使用离散化（对于某些维度使用比较粗粒度的离散化）



## 近似（Approximate）

- $x \approx \theta^T \phi(x)$
- $\phi(x) = feature~ of ~x = \begin{bmatrix} x_1\\x_2\\x_3\\...\end{bmatrix}$

- $V^*(s) \approx \theta ^T \phi(s)$



## Model

- a **simulator** is a **black-box** that takes as input any (continuous-valued) state $s_t$ and action $a_t$ , and outputs a next-state $ s_t+1$ sampled according to the state transition probabilities $P_{s_ta_t}$

![image-20230523131657710](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230523131657710.png)

### 如何构建Model

- open-source软件（off-the-shelf physics simulation software package）

- 物理学家/研究者的帮助（Physics simulation）

- get a model is to learn one from data collected in the MDP.（从数据中进行学习）

  - 使用supervise learning to estimate $s_{t+1}$ 

  - 例如可以通过一个线性模型进行学习：(确定性模型)

    ![image-20230523132802703](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230523132802703.png)

  - 最小化损失

    ![image-20230523133118497](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230523133118497.png)

  - 有些时候真实环境是随机的（stochastic），在学习时添添加噪声$s_{t+1} = As_t+Ba_t +\epsilon_t$

![image-20230523132052090](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230523132052090.png)



## 基于模型的RL

- 基于模型的强化学习算法：通过采集环境数据学习环境模型，并且根据环境模型应用到策略中
- 无模型的强化学习：只通过和环境的交互得到的反馈进行学习，效率较高
- 在传统的游戏和电子游戏中无模型强化学习很好，但是在现实环境真实的物理模型中，基于模型的强化学习模型更加重要



### Fitted value iteration

- **Fitted value iteration** algorithm for approximating the value function of a **continuous state MDP**.

- 基于模型的强化学习模型拥有大量的数据可以学习，不需要担心过拟合的问题，在环境中可以采样到跟泛化的数据来拟合模型

- 复习价值迭代：

  ![image-20230523135043583](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230523135043583.png)

- Fitted value iteration

  - 采样状态$\{s^1,s^2....s^m\}$，采样m种状态

    ![image-20230523135941714](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230523135941714.png)

    （note：有人提问采样这步怎么理解，老师的详细的解释是选出m个状态并计算它们的y值（迭代步）

  - 初始化参数$\theta:= 0$

  - 迭代：

![image-20230523135243342](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230523135243342.png)



- $q(a)$是对价值期望的一个估计
- $y^{(i)}$是对价值迭代更新公式右部的估计



- 确定性模型的k值（采样只需要1次，因为期望变成了某种稳定值）



Q:如何选择m，如何保证不是过拟合

A:![image-20230523140315939](C:\Users\12587\AppData\Roaming\Typora\typora-user-images\image-20230523140315939.png)

因为有了模型，所以不太关心数据集的大小，对于这种强化学习模型，更加关注多少示例不会让模型运算的太慢，m的设置就是一种对于防止过拟合和模型更新速度的权衡



Q:【没听清】

A:![image-20230523140646422](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230523140646422.png)

是的，这里的估计通过之前定义的$\theta^T\phi(s_j)$



Q:是否必须要使用线性模型？

A:这是值得注意的，拟合环境完全不一定是使用线性模型，现在的深度强化学习已经在使用神经网络对模型进行拟合，可以去了解



### Fitted VI计算策略$\pi$

Fitted VI given approximation to V*

![image-20230523141058397](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230523141058397.png)



### 一些技巧

- 模型通常被建模为$s_{t+1} = f(s_t,a_t) + \epsilon$
- 对于模型部署时，将$\epsilon $设置为0,并且将采样样本数k = 1
- 当进入状态S选择价值迭代得到的最大的价值对应的动作

![image-20230523141417896](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230523141417896.png)



Q:自动驾驶和直升机是否可以离散空间

A:这个问题有点深入，但是经验上自动驾驶一般可以使用离散空间，但是对于直升机一般不离散状态空间，可以查阅相关论文

