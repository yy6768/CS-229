# CS 229 （12）

## 课程内容

- backpropagation





## Back Propagation

- 需要寻找 cost function
- 我们使用Batch训练是希望利用GPU的并行能力
- $\mathcal{J} (\hat y, y) = \frac 1 m \sum\mathcal{L}(\hat y , y)$

- 使用logistics regression$\mathcal{L}= -[y^i\log y^i+ (1-y^i)log(1-y^i)]$

- 更新$w^i:= w^i - \alpha \frac {\part \mathcal{J}}{\part w^i}$



如果我们需要计算J对wi的导数，则会消耗大量的计算量，但是根据链式法则可以：

$\frac {\part \mathcal {J}}{\part w^i} = -[y^o \frac {\part}{\part w^i}(\log \sigma(w^ia^j +b^j)) + (1-y^i)\frac {\part}{\part w^i}(log(1-\sigma(w^ia^j+b^j)))]$



梯度形状：

- 可以通过中间层输入的转置来判定梯度的形状

![image-20230425184727588](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230425184727588.png)

- 关于sigmoid的神经网络的反向传播推导

![image-20230425185324451](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230425185324451.png)

![image-20230425190247878](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230425190247878.png)

- Section中将关于链式法则中的向量形状快速确定的技巧



## Cache

- 现在的深度学习框架往往拥有cache技术
  - 框架会分析链式法则并且存储线性函数/激活函数的导数，并在后续的传播计算中使用它们



## 改进网络

1. 使用不同的激活函数

   - ![image-20230425191709479](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230425191709479.png)
   - sigmoid的主要优势：产生的是概率，对分类较好
   - Z值较高的时候，梯度将会变得非常小，容易出现梯度消失
   - tanh同样在饱和时出现这个问题
   - ReLU不容易出现梯度抱着的问题，且计算简单
   - ReLU很适合回归的问题

2. 网络参数normalization

   - 当某一层参数过大时会出现梯度爆炸（sigmoid）

   - 这时候可以引入normalization，降低神经元的输入

   - 这里的normalization不是乘法项，而是归一化（标准化）

     ![image-20230425193412971](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230425193412971.png)

     ![image-20230425193453097](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230425193453097.png)

     更少的迭代

3. 神经网络参数初始化

   1. ![image-20230425194556142](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230425194556142.png)

      当激活函数为sigmoid时，np.random.randn(shape)

      当激活函数为relu时，替换分子为e

      为什么需要随机初始化函数

      ![image-20230425194654298](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230425194654298.png)

   2. Xavier initialization:将参数初始化为$w^{(i)} = \sqrt {\frac{2}{n_{in}^2+n_{out}^2}}$

4. optimization

   为什么minibatch-iteration work？

   - 算法流程

     - 迭代T轮
       - 每轮采样一个batch（一些列参数集合）
       - 前向传播
       - 后向传播，更新参数$w^{(i)},b^{(i)}$

   - batch越小噪声越高（左图batch大，右图batch小）

   - batch越大计算量越大，迭代越慢

     ![image-20230425195516979](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230425195516979.png)

5. momentum algorithm

   假设在某个方向的损失极大

   ![image-20230425195717244](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230425195717244.png)

   我们希望在损失较大的方向（水平方向）进行移动，而削弱垂直于损失方向的方向移动（上下震荡）

   所以如果我们使用动量进行更新，可以消除保持水平方向的移动，而消除反复移动的垂直损失的方向

   RMSProp 和Adam

## 为什么需要激活函数

- 退化成线性回归

Q：我们可以在同一个模型内使用不同的激活函数吗？

A:一般在每一层使用相同的激活函数



## 梯度爆炸和消失（vanishing Exploring）

- 假设有个很深的网络
- 在没有激活函数的情况下，将会变成大量weight项相乘，那么假设weight项几乎相等且大于1，则最后结果呈现梯度爆炸
- 如果系数接近1，则不会爆炸

![image-20230425194027145](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230425194027145.png)

### Explore with 1 neural

![image-20230425194350168](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230425194350168.png)

- 现在一个神经元有多项输入n，n很大
- 我们需要减小w才能防止爆炸
- 所以启发式的来说我们需要$w_i \sim \frac 1 n$



