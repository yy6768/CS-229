# CS229 （20）

## 课程内容

- state-action rewards
- 有限范围MDP
- 线性动态系统（Linear dynamic system）
  - 模型
  - LQR



## State-action rewards

- $R:S\times A \mapsto \R$

- 累积reward ：$R(s_0,a_0) + \gamma R(s_1,a_1), +.... $

- Bellman equation:

  - $V^*(s) = \max\limits_a [R(s,a) + \gamma \sum\limits_{s'\in S} P_{sa}V^*(s')]$

- Value iteration : V(S) := RAS

- 对于最优策略：

  ![image-20230525191614576](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525191614576.png)



## Finite Horizen MDP

- 定义：

![image-20230525191948995](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525191948995.png)

- 注意到时间是有限的，且没有奖励折扣因子$\gamma$
- non_stationary policy:策略取决于时间:$\pi^* (s)$



### Non-stationary state transition

- $s_{t+1} \sim P_{s_ta_t} ^{(t)}$

- 奖励$R^{(t)}(s)$

- $V^*_t(s) = \mathbb{E}[R(s_t,a_t) + R(s_{t+1},a_{t+1})| s_t = s,\pi] $

- 动态规划：

  ![image-20230525193456778](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525193456778.png)

  - 给定最后一步状态对应的reward可以递推前一步

- 算法步骤：

  ![image-20230525193847410](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525193847410.png)

Q:当推广到无限远的时间，是否就能作为正常的值迭代

A:需要折扣系数



## LQR（线性二次型调节器）

- 适用于以下范围：

  - 在finite horizen MDP下更简单

    $\{S,A，P_{sa}, T, R \}$

  - $P_{sa} : S_{t+1} =  AS_t+ Ba_t + \omega_t$(噪声不是最必要的）

    - $A:R^{n*n} , B:R^{d*d}$

- quadratic rewards:

  ![image-20230525194815011](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525194815011.png)

  - $U_t和W_t$是正定举证
  - Remark：当两者都是单位阵的时候，$R_t = - ||s_t||^2 - ||a_t||^2$(这样可以停在平稳状态）
  - 如果需要更改成non-stationary版本只需要将A和B以及U和V接受t作为自变量

- 算法流程：

  - 如何获得A和B矩阵：
    - 根据现实采样，并且最小化：![image-20230525195937067](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525195937067.png)
    - 使用GDA可以学习$\Sigma$



## 学习非线性模型

- 非线性模型定义：

![image-20230525200723067](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525200723067.png)



## 线性化dynamics

- $s_{t+1} = f (s_t) (a_t 被遗忘)$



![image-20230525201702601](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525201702601.png)

- ![image-20230525202138148](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525202138148.png)
- 在某点的前线进行拟合：绿线（切线）在实际上某个范围内（微小的范围）很接近蓝线（真正的dynamics)



![image-20230525202331669](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525202331669.png)

- $s_{t+1} $的这个近似被简化成了一种仿射函数（线性）



## 动态规划问题回顾

我们需要计算Dynamic Programming来得到最佳的$V^*和\pi^*$



![image-20230525203026553](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525203026553.png)

对应笔记的这一部分：

![image-20230525203047976](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525203047976.png)

（因为后一项大于0恒成立，所以简化成只有s的一项）



![image-20230525223126644](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525223126644.png)

- Fact1：如果$V_{t+1}$可以推导出$V^*_t$,那么可以通过相同的表达形式表达$V^*$

- Fact2：

  ![image-20230525223502170](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525223502170.png)

有了上述两个Fact

![image-20230525223522665](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525223522665.png)

- 第二行使用了定义
- 第三行利用了Fact1的猜想

通过某种方式，注释1，得到（真的没搞懂）

![image-20230525223908033](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525223908033.png)

![image-20230525224010574](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525224010574.png)

$\Sigma_t$ 是$w_t$的协方差



总结下来LQR算法的步骤是

![image-20230525224242310](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525224242310.png)



### 一个LQR的特点

- fact3：$L_t$ 中与w无关，也就是它不受噪声的影响，也就是噪声只影响V（价值）不影响$\pi$（决策）

![image-20230525224754174](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230525224754174.png)
