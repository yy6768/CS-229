# CS229 （21）

## 课程内容

- RL debuging
- Policy Search Algorithm
- Conclusion



## 调试RL模型

想要训练无人直升机的控制系统，调试过程：

- 构建一个helicopter
- 构建reward function
- 使用RL算法学习$\pi_{RL}$

然后经常会出现$\pi_{RL}$的效果很差，调试的方法有如下的操作检查：

![image-20230526093721304](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230526093721304.png)

一些诊断方法：

![image-20230526094034120](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230526094034120.png)

- 如果在现实中不好，那意味着是模拟器的问题
- 如果给定人操作，发现人操作的V比算法操作的V大，那么说明强化学习算法有问题
- 如果1，2满足，那么说明是reward/cost function是有问题的



Q: 是否一定要按照1,2,3的顺序进行

A: 上述的方法只是经验，而不是强制的顺序，在现实世界里遇到的问题可能是上述的组合，也有可能比上述问题复杂的多



## Policy Search Algorithm

- 通常先估计$V^* \rightarrow \pi ^*$
- Policy Search 的目的是直接寻找一个$\pi^*$



示例：倒立摆

![image-20230526095236013](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230526095236013.png)

$y \approx h_\theta(x) = \frac 1{1+e^{-\theta^Tx}} $



### 随机策略

- 定义：随机策略是一种函数：$\pi :S \times A \mapsto \R$,描述了当前状态下进行某种动作a的概率$\sum\limits_a \pi (s,a ) = 1$
- $\pi _\theta (s, "R" ) = \frac 1 {1 + e^{-\theta^Ts}}\\ \pi_\theta (s,"L") = 1 - \frac 1 {1 + e^{-\theta^Ts}}$

- 算法目标：寻找$\theta$ 所以执行$\pi_\theta(s,a)$时，最大化$\max\limits_\theta[R(s_9,a_0) + .... R(s_t,a_t)|\pi_\theta ]$

示例，假设t = 1:

![image-20230526140914259](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230526140914259.png)



## Reinforce algorithm

- 核心思想：
  - 采样$s_0,a_0,s_1,a_1,....,s_T,a_T$
  - 计算payoff$R(s_0) +R(s_1)+ ....+R(s_T)$
  - $\theta := \theta + \alpha [\frac {\Delta_\theta \pi_\theta(s_0,a_0) }{\pi_\theta (s_0,a_0)} + \frac {\Delta_\theta \pi_\theta(s_1,a_1) }{\pi_\theta (s_1,a_1)} + ... + ]\times payoff$

- 是一种梯度下降算法

- 这里提及了一些代数细节，包括上面的梯度下降怎么计算得到的（以T=1）为例

![image-20230526142417615](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230526142417615.png)

- 可以看到：	
  - 利用链式法则将乘积的导数展开
  - 同时称一项$\pi_\theta(s_t,a_t)$，使得可以提出公因式（橙色框）
  - 最终说明对目标函数的求导就是常数$P(s_0,a_0,s_1,a_1)$乘上我们需要的梯度下降的值，也就是梯度下降的方向就是我们对目标函数优化的方向

Q:听不清

A:与特征选择完全无关



Q:

A:当然，另一种方式就是使用supervise learning，通过人类中的专家的动作，让机器学习，但是对于直升机这种控制系统，一些环境变换是不可预见的，所以通常使用强化学习得到的模型比人类专家的效果还好



连续空间的随机策略

- $a = \theta ^T s $ + gauusian noise





### 价值迭代Vs 直接策略搜索

适合直接使用策略搜索的情况

- 当有一个POMDP（部分被观测的MDP）
  - 使用Cartpole没有足够的传感器，此时只能关注到状态的子集）

![image-20230526143636824](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230526143636824.png)

$\pi(y, "R") = \frac {1}{1 + e^{-\theta^Ty}}$



- reinforce algorithm 效率很低
- 直接策略搜索的另一种情况是：$V^*$简单还是$\pi^*$简单
  - 低级控制（$\pi^*)$还是高级控制($V^*$)
  - 直升机/驾驶汽车是典型的低级控制任务（人类的操作都是本能的，大部分时间聚焦于少数状态之后）
  - 下围棋是高级控制（必须考虑多个步骤，也就是要进行多个状态之后的运算）





## 课程总结

- 监督学习
- 学习理论
- 无监督学习
- Kmeans ，PCA，ICA，混合Gaussian
- 值迭代，Fitted值迭代，Reinforce……（强化学习）

