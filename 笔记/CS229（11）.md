# CS229 Lecture 11

## 课程内容

- Logistic Regression复习
- Neural Network



## 深度学习

- 深度学习
  - computational power
  - data available
  - 算法

### 问题1：

- 图像分类：
  - 问题：图像中是否有猫
  - 输入64*64\*3的RGB图像
  - 将图片展平（flatten)
  - 通过线性加权得到$wx+b$
  - 使用$sigmoid$函数

![image-20230416232716112](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230416232716112.png)

- 训练：
  - 初始化参数w和b
  - 优化w和b
    - 损失函数$ylog\hat y +(1-y)log(1-\hat y)$
    - 求偏导数进行迭代
  - 进行预测
- 神经元neuron：linear + activation
- module： architecture + parameter
- model = architecturer + parameter



### 问题2

- 图像分类：
  - 找到/cat/lion/iguana(多分类问题)
  - ![image-20230417160024731](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230417160024731.png)
  - 与上一个问题不同的是，需要分类的物体更多了，所以有3个输出层
  - 参数：64*64 \* 3 * 3(64 * 64是图像大小，3个RGB通道，3个输出)

- 这样的神经网络是鲁棒的吗？
  - 是的，因为三个神经元彼此不交涉，这又就造成了每一个神经元只关心他的网路

![image-20230417161045694](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230417161045694.png)

### Softmax（问题3）

- 图像分类：只在图像中预测出一种动物

- $softmax(z_i) = \frac {e^{z_i}}{\sum\limits_{j = 1}^n e^{z_j}}$
- softmax得到的是一种概率分布



## 神经网络训练

- 直观上，如果采用问题1中的Loss，进行改进：$-\sum\limits_{k = 1}^{|z|} [y_k log\hat y_k+(1- y_k)log(1-\hat y _k)]$

Q:听不清

A:通常是这样的。该网络一旦训练，最后输出一个概率分布

如果选用上述的Loss：

- 当计算$w_2$的导数时，情况将会变得很复杂，因为不仅$w_2$的导数不仅涉及$w_2$还涉及$w_1,w_3$

- softmax entropy loss:
  - $L = -\sum_{i =1}^zy_klog\hat y_k$



## nerual network

### 问题4

- 问题4 ：如果我们需要做的不是预测是否有猫，而是预测猫的年龄会怎么办？
- 回答：是有第三个网络，对每一个年龄都使用一套w和b，最后使用softmax
- 或者我们直接使用回归：
  - 将sigmoid替换为ReLU函数
  - 将损失函数替换为MSE或者MAE



### 问题5

- 输入图像，得到cat或者no cat，我们希望模型能够有极高的准确度
- 解决方法：使模型更加复杂，添加一层隐藏层（hidden layer)

![image-20230417182013606](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230417182013606.png)

- 参数大小：

  - 第一层：3n+3
  - 第二层：2*3 + 2
  - 第三层：2*1 +1

  （并没有添加太多的参数）



- 隐藏层能够学习到一些抽象：
  - 研究者并不知道隐藏层具体会做什么
  - 但是隐藏层确实可以获取一些抽象信息：比如识别猫的模型中耳朵和嘴巴的特征，最后输出层构成猫的面孔特征



### 问题6

- 房价预测：

  特征

  - 我们有房子的大小
  - 邮政编码
  - wealth

  ![image-20230417182814939](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230417182814939.png)

- 神经网络可以提取出抽象信息：比如学校资源，工作环境等

- 神经网络是一个“黑盒模型”，end-to-end模型



## Propagation equation

![image-20230417183232982](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230417183232982.png)

- 随机梯度下降的方向过于随机，所以每一次只代表某一个样本的下降方向
- 梯度下降的方向比较精确，根据



### 向量化

![image-20230417183859790](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230417183859790.png)

- $z^i = w^i x +b^i$

- 广播技术

  - 现代框架基本都支持

  - 问题 ：

    ![image-20230417184256223](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230417184256223.png)

  - wx的size和b不匹配，我们可以在并行计算的过程中，将b重复m次，变成3*m的大小，使得矩阵维度匹配



Q:这与主成分分析有什么不同

A:主成分分析是非监督学习，只能进行特征降维，让你知道最重要的特征，而神经网络只有输出



Q:能解释一下为什么第一层能看到边缘？

A:并不能总是看到边缘，以人脸检测为例，神经网络看到的像素，由于只能看到像素，他无法理解整体的信息，但是边缘因为差距很大，所以第一层获得的总是边缘信息。这是一个高端问题，现在有很多前沿的研究。具体复杂模型获得了什么抽象，还需要等待研究者发现。



 Q：如何设计神经网络架构？

A:通过测试和验证，调整架构，其次通过过往对模型的经验。直觉就是问题越复杂，使用的神经网络越深。



## Optimization

- 需要优化大量参数
- 损失函数Loss和cost function
  - $Loss(y,\hat y) = \frac 1 m\sum\limits_{i =1}^m \mathcal{L^i}$
  - $\mathcal{L} = [y^ilog\hat y^i + (1 - y^i)log(1- \hat y ^i)]$

- 求导：是对Loss求w2的导数难还是对$\mathcal{L}$求w2的导数难：等价的，根据链式法则

### Backward propogation

![image-20230417185738224](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230417185738224.png)

- 求w1（第一层的权重)导数难还是求w3（第三层的权重）的导数难

  - 因为w3最接近输出，所以计算w3的导数只需要一步，而w1的导数涉及w2和w3

  ![image-20230417190120366](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230417190120366.png)

- 可以看到我已经计算出了$\frac {\part y}{\part z_3}$,所以在计算w2的导数时，我们不想重复计算，所以使用链式法则，可以直接得到w2导数的一部分