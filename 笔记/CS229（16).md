# CS229 Lecture 16

## 课程内容

- Principal Component Analysis（PCA）



## PCA

- PCA并不关注模型（只关注数据的特征维度），而factor analysis是提前假设好了模型

- 案例

例如长度和inch特征

![image-20230502092741019](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230502092741019.png)

- 他们基本可以以线性关系进行拟合（有一定的噪声）

- 所以我们可以去除这些高度线性相关的维度之一



### 流程

标准化

- $\mu = \frac 1m \sum\limits_{i=1}^m x^{(i)}$
- $x^{(i)} -= \mu$
- $\sigma^2 = \frac 1 m \sum_{i}x^{(i)^2}$

- $x /= \sigma$



![image-20230502094421833](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230502094421833.png)

- PCA 希望我们寻找到绿色的线性关系（有着最小二乘距离）

- 等价于在线上的投影的距离最远，绿线其实就是新的维度



- PCA 目标$max _{u;||u||= 1} \frac 1m \sum\limits_{i= 1}^m (x^{(i)T}u)^2 = u^T(\frac 1 m \sum\limits_{i= 1}^m x^{(i)}x^{(i)T})u$

- 解释u是绿线的单位向量$x^{(i)T}u$其实就是绿线上的投影距离

-  $max_{u;||u||=1} u^T\Sigma u$ 

  - $\Sigma$是经验协方差矩阵
  - 为了满足最大化，u应该选择为principal eigenvector
  - 拉格朗日乘子法

  ![image-20230502095453701](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230502095453701.png)

  - 解得：

    ![image-20230502095645282](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230502095645282.png)

- 如果希望将数据从n维降维为k维，那么就选择$(u_1,u_2,u_3...,u_k)$（top k个特征向量），对应top k个特征值($\lambda_1, \lambda_2,...,\lambda_n$)



Q:如何选择k？

A:之后会介绍到

Q:为什么要标准化

![image-20230502101137168](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230502101137168.png)

A:对于这样的数据实际可以标准化为同一维数据，只是它们的规模不一致，预处理的主要作用就是统一度量



### PCA的应用

- Visualization：将数据从n维降维到2D或者3D
  - 猴子脑机接口，了解猴子如何思考触屏游戏

- compression for ML learning efficiency

- 减少维度来减少overfitting
- outline detection （轮廓检测）

![image-20230502103325515](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230502103325515.png)



### 非监督学习的阶段性总结

![image-20230502104605852](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230502104605852.png)

- 我们将模型分为概率型和非概率型
  - 将目标分为降维和聚类

Q:如何选择k

A:保有90%以上的数据的方差

![image-20230502105300658](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230502105300658.png)

更有甚者持有95%或者98%





## ICA（independent component analysis）

- 问题，话筒采集两个人的声音，导致混淆在一起

![image-20230502105703818](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230502105703818.png)

- ICA可以将两个人的声音分离



![image-20230502110351477](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230502110351477.png)

- 声音信号大致如下，左右分别表示两个话筒采集的信号量

- 我们能够观察到：

  - $x^{(i) }= As^{(i)}, x^{(i)}\in \R^n$

  采样x应该是两个声音s的某个线性组合，A被称为 **mixing matrix**，

![image-20230502111215108](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230502111215108.png)

- 可以使用A的逆矩阵来还原出s
- $W = A^{-1}$被称为**unmatrixing matrix**可以通过$s^{(i)} = Wx^{(-1)}$

![image-20230502111632326](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230502111632326.png)