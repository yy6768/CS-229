# CS229（5）

## 课程内容

- GDA
- 广义判别模型
- 朴素贝叶斯



## 生成模型

- 生成模型：相较于判别模型中划分不同的数据点，生成模型会对每种类别的数据点的分布进行建模



- Discriminative 
  - 针对 p( y | x) 进行学习 或者学习$h_\theta(x) = 0\ or\ 1$

- Generative
  - 针对$p(x|y)$进行学习， 同时学习p（有）

## Bayes Rules

$p(y | x) = \frac{p(x|y)  p(y)}{p(x)}$





## GDA(Gaussian Discriminate Anaysis)

### 基本概念

- 问题概述：
  - $x\in \R^n（x_0= 1）$
  - 假设$p(x|y)是高斯分布$
- 多元高斯分布
  - $z \sim  \mathcal{N}(\mu, \Sigma)$
  - $\mu \in \R^n ~ \Sigma \in \R^{n\times n} $
  - $E[z] = \mu$
  - $Cov[z] = E[zz^T] - (E[z])(E[z])^T$
  - $p(z) = \frac 1{(2\pi)^\frac {n}{2} |\Sigma|^\frac 1 2} exp (-\frac 1 2 (x-\mu) ^T \Sigma ^{-1} (x-mu))$
  - note中展示了当$\Sigma$越大时，极点越低，越高是极点越高
  - 当$\Sigma$为对角阵时，元素两两不相关，横截面为圆，当元素有相关性，那么横截面为椭圆



### GDA模型

- GDA模型对$p(x | y = 0) 和 p(x | y = 1)$建模
- 参数:$\mu_0, \mu_1, \Sigma,\phi$
- 一般假设两个类具有相同的$\Sigma$
- $p(y) = \phi ^y (1 - \phi)^{1-y}$



### 训练

- 最大化联合似然（joint likelyhood）

  $L(\phi, \mu_0,\mu_1, \Sigma) = \prod \limits_{i = 1} ^ {m} p (x^i,y^i,\phi,\mu_0,\mu_1,\Sigma)\\=\prod \limits_{i = 1} ^ {m}p(x^i|y^i) p(y^i)$

  - 区别于判别式模型的最大化条件似然

    $L(\phi, \mu_0,\mu_1, \Sigma) = \prod \limits_{i = 1} ^ {m} p (y^i |x^i,\phi,\mu_0,\mu_1,\Sigma)$

- 训练最大化参数
  - $maxarg\ L(\phi, \mu_0,\mu_1, \Sigma)$
  - $\phi = \frac {\sum\limits_{i = 1} ^ m \mathcal{L}\{y^i = 1\}} m$
  - $\mu_0 =\frac{ \sum\limits_{i = 1} ^ m \mathcal{L} \{y^i = 0\} x^i} {\sum\limits_{i = 1} ^ m\mathcal{L} \{y^i = 0\}}$
  - $\mu_1 =\frac{ \sum\limits_{i = 1} ^ m \mathcal{L} \{y^i = 1\} x^i} {\sum\limits_{i = 1} ^ m\mathcal{L} \{y^i = 1\}}$
  - $\Sigma = \frac 1 m \sum\limits_{i = 1} ^m (x^i - \mu_{y^i})(x^i - \mu_{y^i})^T$



### 预测

- 选择最大的y对应的分类$max_{y} p(y | x) = max_y \frac {p(x|y) p(y)}{p(x)} = max_y p(x|y)p(y)$



### 对比

- 分割线：判别模型线性回归（Logistic回归略有不同）
- 等高线：生成模型

![image-20230319135845181](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230319135845181.png)



Q：为什么不使用两个$\Sigma$参数代表两个类

A: 

- 会使参数翻倍
- 得到的将不会是线性边界，不是非常合理



## GDA对比于逻辑回归（logistic regression)

### 两者之间的联系

![image-20230319140742306](C:\Users\12587\AppData\Roaming\Typora\typora-user-images\image-20230319140742306.png)

- 利用其中一个高斯模型:$p(y = 1|x)$在数轴上进行绘制，得到的映射关系就是Sigmoid函数（PS1习题）





### 不同问题的适用范围

- GDA 假设
  - $p(x|y = 0) \sim \mathcal{N} (\mu ,\Sigma) $
  -  $p(x|y = 1) \sim \mathcal{N} (\mu ,\Sigma) $
  - $y \sim Ber(\phi)$
- Logistic Regression 假设
  - $p(y = 1 |x) = \frac 1 {1 + e^{-\theta^Tx}}$
- GDA假设是Logistic Regression的充分假设，也就是GDA可以推出Logistic 的假设，但是反之不然
- GDA是更强的假设，如果给定的假设满足GDA假设，GDA的效率更高，选择GDA更好
- 但是如果不满足GDA假设，那么智能选择Logistic Regression



- $x | y =1 \sim Possian(x)$
- $x | y = 1 \sim Possioan(x)$
- $y \sim Ber(\phi)$

同样也可以推出logistic回归的假设



- 更小的训练集更适合GDA
- 如果适用Regression ，模型将会更加健壮



Q:如果协方差矩阵不同的两个类使用Logistic回归会如何

A:那么Logistic Regression的假设就不能是线性的了，最终得到的边界将会是非线性边界

Q：是否可以先训练一次GDA看效果如何再进行决定

A：高维数据的情况非常多。一般如果有足够的数据的话训练Logistic regression 模型。，但是这是一种选择问题，最终取决于哲学思考和经验。数据量大时，不同水平的数据团队产生的模型性能差距会减少。数据量较少时，更考验团队的技术和经验



Q：GDA是否能处理多分类问题

A：同样适用。





## Native Bayes

- 问题：电子邮件标记为垃圾邮件
- 步骤：
  - 手机词汇表
  - 根据是否出现将词汇标记成特征向量（$x\in \{0,1\}^n=>参数过多$

使用native Bayes

- bayes假设：每一个$x_i$条件独立于y
- $p(x_1, x_2 ,x_3 ,……,x_{10000}|y) = \prod\limits_{i = 0} ^ m p(x_i|y)$

- 参数：
  - $\phi _{j |y = 1} = p(x_j = 1 | y = 1)$
  - $\phi _{j|y = 0} = p(x_j = 1 |y = 0)$
  - $\phi _y = p(y = 1)$
- 训练：
  - $极大化联合似然\mathcal{L}(\phi_y,\phi_{j|y = 0}, \phi_{j | y =1}) $
    - $\phi_y = \frac {\sum\limits_{i =0} ^m \mathcal{L}\{y ^i = 1\}} m $
    - $\phi_{j | y =1} = \frac {\sum\limits_{i = 1} ^m \mathcal{L}\{x^i_j = 1, y^i = 1\}}{\sum\limits_{i = 1} ^m \mathcal{L}\{y^i =1\}}$
    - $\phi_{j | y =0} = \frac {\sum\limits_{i = 1} ^m \mathcal{L}\{x^i_j = 1, y^i = 0\}}{\sum\limits_{i = 1} ^m \mathcal{L}\{y^i =0\}}$