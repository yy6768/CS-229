# CS229 （17）

## 课程内容

- CDFs
- ICA model

- 强化学习
  - MDP



## ICA

- 我们有多个麦克风进行采样
- $x = As$
- 我们的目标是寻找矩阵$W = A^{-1}$



ICA与高斯分布

- 如果你的模型是高斯分布，那么就无法进行ICA
  - 高斯分布具有旋转对称性
  - 只有高斯分布具有旋转对称性（标准对称）



- ICA假设数据样本是非高斯的
- Ps(s)



### CDF

- CDF:累积分布函数（累积分布函数）
- F(s) = $P(S \le s)$
  - S是随机变量
  - s是常量

![image-20230504192914452](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230504192914452.png)

- $P_s(s) =F'(s)$(PDF是CDF的一阶导数)

- PDF：概率分布函数



- $x = As = W^{-1}s => s=Wx$
- $p_x(x) = p_s(Wx)(错误)$
- 为什么错误？举例：
  - $p_s = I\{0 \le s \le 1\} \\x = 2s$
  - $p_x = \frac 1 2 I\{0 \le x  \le 2\}$对应源公式
- $p_x(x) = p_s(Wx)|W|$

### ICA算法

- 整个声源的分布是单个声源分布的联合分布$p(s)  = \Pi_{i=1}^n p_s(s_i)$

- 单个样本x的PDF$p_x(x) = p_s(Wx)|W| = \prod\limits_{i = 1}^n p_s(w_i^Tx)\cdot|W|$

- 极大似然法：$\mathcal{l} \sim \sum_{i = 1}^mlog(\Pi_j p_s(w_j^Tx^{(i)}))|W|$

- 使用随机梯度下降

  ![image-20230504195514693](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230504195514693.png)



### 应用

![image-20230504201033872](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230504201033872.png)

- 清理脑电波数据

  ![image-20230504201200916](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230504201200916.png)

- ICA处理微观中的自然图片



## 强化学习

- 强化学习不会显式的指示控制任务的操作，而是涉及一系列函数（奖励函数）来让智能体从奖励中学习

- 强化学习的难点是分配问题（assignment problem），当失败时（50步）应该判断在哪一个出现问题

### Notation

- R(s)
  - R是奖励函数
  - s是状态

### MDP(Markov Decision processes)

- $S,A,\{P_{sa}\},\gamma, R$
  - S-状态集合
  - A-动作集合
  - $P_{sa}$-状态转移概率（state-trainsition）
  - $\gamma$-discount factor
  - R-reward function
- 案例：
  - 11个状态
  - 上下左右四个动作

![image-20230504233233038](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230504233233038.png)

- 每一步需要消耗一定的cost：不然机器人会消耗电力
- 累积奖励:$R(s_0) + \gamma R(s_1) + \gamma^2R(s_2)+......，\gamma$用于激励更早的获得奖励

![image-20230504233524223](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230504233524223.png)

- 目的：从动作空间中选取动作最大化期望总收益$\mathbb{E}[\sum\limits_t\gamma^tR(s_t)]$



- Policy:$\pi :S\rightarrow A$
