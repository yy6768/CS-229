# Lecture 4 

- Perception
- Exponential family
- Generalized Linear model
- softmax regression



### perception （感知机）

- $Sigmoid函数： g(z) = \frac 1 { 1 + e^{-z}}$
- $$ g(z)=\begin{cases}
  1 & z>0 \\
  0 & z<0 \\ 
  \end{cases}$$

- $h_\theta(x) = g(\theta^Tx)$  

- $\theta_{j + 1} = \theta_j + \alpha (y^i - h_\theta(x)^i)x_j^i$

![image-20230314225011471](C:\Users\12587\AppData\Roaming\Typora\typora-user-images\image-20230314225011471.png)

假设有一个类误分类，例如图中的x（在边界下方），此时我们想做的是让法向量$\theta$靠近x

假设有一个类正确分类，那么反之，我们希望$\theta$ 远离



Q:为什么不在实践中运用感知机

A:

- 没有概率解释
- 不能解决异或问题

## Exponential family

- 定义：概率密度函数可以写作$p(y;\eta) = b(y) exp[\eta^T T(y) - \alpha (\eta)]$的函数
  - $y$ -data
  - $\eta - 自然参数$
  - $T(y) - suffient\ statistic$,一般是y
  - $b(y)$  - Base measure
  - $a(\eta) -$ log partition

- 高斯函数也属于Exp family



#### Bernoulli 分布（二项分布）

- 符号$\phi$
- pdf(概率密度函数）：$$p(y;\phi) = \phi ^ y (1-\phi) ^{1-y}\\ =exp(log(\phi^y(1-\phi)^{1-y}))\\=exp[log(\frac \phi {1- \phi})y + log(1-\phi)]$$
  - b(y) = 1
  - $\eta = log(\frac \phi {1- \phi})$
  - T(y) = y
  - $\eta = log(1-\phi) => \phi = \frac 1 {1 + e^{-\eta}}$
  - $a(\eta) = -log(1-\phi) =>log(1 +e^{-\eta})$



#### Gaussian (恒定方差)

- assume:$\sigma^2 = 1$
- pdf:$p(y;\mu) = \frac 1 {\sqrt{2\pi}} exp(-\frac{(y-u)^2} 2) \\= \frac 1 {\sqrt{2\pi}}e^{-\frac {y^2} 2} exp(\mu y - \frac 1 2 \mu^2)$
  - b(y) = $\frac 1 {\sqrt{2\pi}}e^{-\frac {y^2} 2}$
  - T(y) = y
  - $\eta = \mu$
  - $a(\eta) = \frac {\mu^2} 2$

### 常见

- Real ：高斯
- Binary： Bernoulli
- Count：泊松
- $R^+ -Gamma Exponential$
- Distribution（概率分布）：Beta，Dirichlet 

#### 数学特性

1. eta的极大似然函数是凹函数=>NLE (负的极大似然函数（损失函数的变式)） 是凸函数
2. $E[y:\eta] = \frac \part {\part n} a(\eta)$
3. $Var[y;\eta] = \frac \part {\part n ^2}a(\eta)$



## GLM(广义线性模型)

1. 假设：

   1. 给定$y|x;\theta \sim Exponential\ falmily$
   2. 给定x,我们的目标是预测T(y)的期望值，一般$T(y) = y$,记$h(x)$是我们的预测值，$h(x) = E[y|x] $ ,特别的对于二分类问题，$h(x) = p(y = 1|x;\theta)$
   3. 指数族函数的自然参数（natural param)与x呈线性相关=> $\eta = \theta^T x, \theta \in \R^n, x \in  \R^n$

2. 理解：模型的真实值满足某种概率密度函数为指数组的概率分布

   ![image-20230315111853249](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230315111853249.png)

3. 如何训练模型:$max _\theta log \ p(y^i;\theta^Tx^i)$

   - $\theta_{j + 1} = \theta_j + \alpha (y^i - h_\theta(x)^i)x_j^i$

4. terminology:

   1. $\eta$:自然参数
   2. $E[y;\eta] = g(\eta) $->规范响应函数(canonical response function)
   3. $\eta = g^{-1}(E) -> $规范链接函数（canonical link function）
   4. $g(\eta) = \frac \part {\part\eta}a(\eta)$
   5. 参数术语：
      1. model param :$\theta$（x做的线性变换的参数，需要学习的参数）
      2. natural param :$\eta$（分布的参数）
      3. canonical param : 
         1. $\phi$ - Bernoulli
         2. $\mu\ \sigma^2$ -Gaussian
         3. $\lambda$ - Poisson 

Q:如何选择分布

A:取决于任务。比如任务是回归，那么回归是实数域的问题，那么高斯分布就比较适合，例如任务是分类，那么就是二进制问题，使用Bernoulli分布就较好，如果任务是网站浏览量等计数类问题，那么就可以用泊松分布





$h_\theta(x) = E[y|x;\theta] = \phi = \frac 1 {1 + e^{-\eta}} = \frac 1 { 1 + e^{-\theta x}}$

#### Assumptions

- regression

![image-20230315114624223](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230315114624223.png)

在y上有不同的方差为1的高斯分布，我们根据实际的数据得到自然参数$\eta$的极大似然，反向推导$\theta$



PS左图是模型 右图是真实数据分布

- 分类

![image-20230315115012327](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230315115012327.png)

我们使用sigmoid函数，最终可以得到一个Bernoulli分布，根据这个Bernoulli分布的自然函数，根据更新反向计算$\theta$





## Softmax Regression

- 问题：多分类问题，给定一个点，如何判断它是什么类

![image-20230315115222176](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230315115222176.png)

- notation：
  - k-class的数量
  - $x^i \in \R^n$
  - $label(类标签)： y=[\{0, 1\} ^k]$

- 如果我们使用广义线性模型作用与多分类

  ![image-20230315120216284](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230315120216284.png)

  得到多条划分直线，我们对于每一条划分直线，会得到一个x和class的关系：

![image-20230315121851516](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230315121851516.png)

经过指数变换：

![image-20230315122030205](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230315122030205.png)

经过归一化：

![image-20230315122049149](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230315122049149.png)



- 交叉信息熵：
  - $CrossEnt(p,\hat p) = \sum\limits_{y\in \{0,1,……,k\}}p(y)log \hat p(y) \\= -log \hat p (y_k) \\= -log \frac{e^{\theta_k^T x}}{\sum\limits_{y\in \{0,1,……,k\}} e^{\theta_c^Tx}} $