# CS229 （18）

## 课程内容

- MDP
- Value function
- Value /policy iteration
- Learning $p_{sa}$



## MDP

- MDP从$s_0$开始,进行$a_0$,根据状态转移概率得到$s_1$,得到reward$R(s_0)$,逐渐迭代

- 上一讲Lecture17 介绍了如何选取最佳策略



## RoadMap

- 为了寻找最佳的策略，我们需要寻找

  - $V^\pi $: 

    $对于策略\pi ,V^\pi :S->R~ V^\pi(s)表示从s开始的累积奖励的期望，称作值函数\\V^\pi = \mathbb{E}[\sum\gamma ^iR(s_{i})|\pi,s_0=s]$

    ![image-20230505092846796](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230505092846796.png)

  - $V^*$:最优值函数$V^* = max_{\pi}V^\pi(s)$



### Bellman Equation

- $\begin{aligned}V^\pi(s) &= E[R(s_0) + \gamma \sum\limits_{s,\pi(s')}V^\pi(s')]\end{aligned}$

- $s' \sim P_{s, \pi(s)}$

- 可以发现给定$\pi$,V(s)是其他所有s'的价值函数$V^\pi(s')$的线性组合

- $V^* = R(s) + max_a \gamma \sum\limits _{s'} P_{sa}(s')V^*(s')$

- $\pi^*(s)= argmax_{a} \sum\limits_{s'}P_{sa}(s')V^*(s')$
  - 根据$V^*$的推导得出
- $V^*(s) =V^{\pi^*}(s)\ge V^\pi(s) $



构造最优策略：

- 计算$V^*$
- 使用argmax方程寻找$\pi^*$





## 价值迭代

- 初始时设置V(s):=0 对于所有状态s
- 每一次更新：

![image-20230505101211571](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230505101211571.png)

- Syndrome update/Asyndrome update
  - 同步更新：每一次更新多维向量中存储的所有价值函数值
  - 异步更新：每一次根据当前的其他值更新一个值函数值，不断进行迭代
  - 同步更新可以利用向量化运算，所以当前一般使用同步更新

- 价值更新最后可以得到最优价值函数$V^*$



## 策略迭代

- 随机初始化$\pi$
- 迭代：
  - $V^\pi$覆盖V
  - 利用Bellman方程进行更新

![image-20230505105356480](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230505105356480.png)

- 与价值迭代核心不同点在于，每次迭代都会产生新的策略，且价值迭代先计算最优价值函数，再计算策略
- 通过线性方程组解方程求解V



## 状态转移方程

- 如果我们事先不知道状态转移方程$P_{sa}$
- $P_{sa}(s')$ 估计为访问状态s采取动作“a"并且达到状态s'在状态s整个动作转移中的次数占比（可能需要拉普拉斯平滑）
  - 但是MDP问题对于0并不敏感，所以多数时间不会使用laplace 平滑

![image-20230505110839012](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230505110839012.png)



## 完整的强化学习训练过程

- 价值迭代

  ![image-20230505111135411](http://typora-yy.oss-cn-hangzhou.aliyuncs.com/img/image-20230505111135411.png)

  - 执行$\pi$进行估计
  - 先估计$P_{sa}$
  - 应用价值迭代
  - 更新$\pi$



## 探索和利用（Exploration vs Explanation）

- $\epsilon - greedy$: $\epsilon$的概率进行探索，剩余情况选择最优